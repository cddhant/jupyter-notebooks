{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b> Problem Statement </b>\n",
    "1. Extract Sample document and apply following document preprocessing methods:<br>\n",
    "Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization.<br><br>\n",
    "2. Create representation of document by calculating Term Frequency and Inverse Document \n",
    "Frequency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b> 1. Tokenization of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')\n",
    "# OR\n",
    "# import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello everyone.', 'My name is Siddhant.', 'Today is my DSBDA practical exam.', 'This is sentence tokenization']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Hello everyone. My name is Siddhant. Today is my DSBDA practical exam. This is sentence tokenization\"\n",
    "\n",
    "tokenized_sentence = sent_tokenize(text) \n",
    "\n",
    "print(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting sentence in words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', '.', 'My', 'name', 'is', 'Siddhant', '.', 'Today', 'is', 'my', 'DSBDA', 'practical', 'exam', '.', 'This', 'is', 'sentence', 'tokenization']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "  \n",
    "text = \"Hello everyone. My name is Siddhant. Today is my DSBDA practical exam. This is sentence tokenization\"\n",
    "\n",
    "tokenized_word = word_tokenize(text)\n",
    "\n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b> 2. POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parts-Of-Speech Tagging for the following sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', 'NNP'),\n",
       " ('everyone', 'NN'),\n",
       " ('.', '.'),\n",
       " ('My', 'PRP$'),\n",
       " ('name', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('Siddhant', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('Today', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('my', 'PRP$'),\n",
       " ('DSBDA', 'NNP'),\n",
       " ('practical', 'JJ'),\n",
       " ('exam', 'NN'),\n",
       " ('.', '.'),\n",
       " ('This', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('sentence', 'JJ'),\n",
       " ('tokenization', 'NN')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "text = \"Hello everyone. My name is Siddhant. Today is my DSBDA practical exam. This is sentence tokenization\"\n",
    "\n",
    "postag = word_tokenize(text)\n",
    "\n",
    "nltk.pos_tag(postag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b> 3. Stop Word Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop Words: A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are the stop words in english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'should', 'ma', 'hadn', 'hasn', 'than', 'herself', \"wouldn't\", \"you're\", 'theirs', 'are', 'won', \"it's\", 'below', 'were', 'above', 'why', 'no', 'will', 'her', 'itself', 'shouldn', 'if', 'you', 'himself', 'between', 'mustn', 'does', 'your', 'needn', 'aren', 'y', 'was', 'off', 'own', 'such', 'is', 'ain', 'those', 'a', 'once', 'am', 'other', 'yourselves', 'she', \"should've\", 'he', 'by', 'more', 'over', 'as', \"don't\", 'the', 'because', \"she's\", 'they', 'from', 'there', 'i', 'me', 'doesn', 'now', 'couldn', 'some', 'been', 'most', \"weren't\", 'my', 'whom', 'just', 'ourselves', 'did', 'haven', 'be', 'during', 'wouldn', \"mightn't\", 'our', 'themselves', 'him', 'these', 'which', 'where', 've', 'all', \"isn't\", 'them', \"you'd\", \"shan't\", 's', 'an', 'for', 'isn', 'or', 'too', 'what', 'myself', 'but', 'few', 're', 'before', 'll', 'yourself', 'being', 'it', 'has', 'wasn', 'not', \"shouldn't\", 'on', \"hasn't\", \"doesn't\", \"haven't\", 'here', 'can', 'after', 'had', \"hadn't\", 'out', 'in', 'hers', \"you've\", 'don', \"needn't\", 'with', 'yours', \"that'll\", 'didn', 'about', 'his', 'until', \"didn't\", \"wasn't\", 'through', 'further', 'their', 'of', 'same', 'up', 'd', 'while', 'do', 'this', 'ours', 'how', 'each', 'very', 'weren', 'we', 'when', 'into', 'm', 'down', 'only', 'having', 'who', 'that', 'mightn', 'then', 'against', 'under', \"aren't\", 't', \"couldn't\", 'doing', 'to', 'have', 'both', \"you'll\", 'and', 'at', 'so', 'again', \"mustn't\", 'shan', \"won't\", 'nor', 'any', 'its', 'o'}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentence:  ['Hello', 'everyone', '.', 'My', 'name', 'is', 'Siddhant', '.', 'Today', 'is', 'my', 'DSBDA', 'practical', 'exam', '.', 'This', 'is', 'sentence', 'tokenization']\n",
      "Filtered Sentence:  ['Hello', 'everyone', '.', 'My', 'name', 'Siddhant', '.', 'Today', 'DSBDA', 'practical', 'exam', '.', 'This', 'sentence', 'tokenization']\n"
     ]
    }
   ],
   "source": [
    "filtered_sentence = []\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)    # append = add\n",
    "\n",
    "print(\"Tokenized Sentence: \", tokenized_word)\n",
    "print(\"Filtered Sentence: \", filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b> 4. Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lowering the inflection in words to their root forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:  My name is siddhangt. Send sending sent\n",
      "output:  my name is siddhangt. send sending s\n",
      "Filtered Sentence:  ['Hello', 'everyone', '.', 'My', 'name', 'Siddhant', '.', 'Today', 'DSBDA', 'practical', 'exam', '.', 'This', 'sentence', 'tokenization']\n",
      "Stemmed Sentence:  ['hello', 'everyon', '.', 'my', 'name', 'siddhant', '.', 'today', 'dsbda', 'practic', 'exam', '.', 'thi', 'sentenc', 'token']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "example = \"My name is siddhangt. Send sending sent\"\n",
    "output = ps.stem(example)\n",
    "print(\"Example: \", example)\n",
    "print(\"output: \", output)\n",
    "\n",
    "\n",
    "stemmed_words = []\n",
    "\n",
    "for w in filtered_sentence:\n",
    "    stemmed_words.append(ps.stem(w))\n",
    "\n",
    "print(\"Filtered Sentence: \", filtered_sentence)\n",
    "print(\"Stemmed Sentence: \", stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b> 4. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n",
      "disfunctional : disfunctional\n",
      "disfunctional : disfunctional\n",
      "better : good\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    " \n",
    "lemmatizer = WordNetLemmatizer()\n",
    " \n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
    "print(\"disfunctional :\", lemmatizer.lemmatize(\"disfunctional\"))\n",
    "print(\"disfunctional :\", lemmatizer.lemmatize(\"disfunctional\"))\n",
    " \n",
    "# a denotes adjective in \"pos\"\n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('My', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('John', 'NNP')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "text = \"My name is John\"\n",
    "\n",
    "postag = word_tokenize(text)\n",
    "\n",
    "nltk.pos_tag(postag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c79b7ad1551a3a1301c544711332fa021a07e83fcacf5ad63ca5d8fc456c0c33"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
